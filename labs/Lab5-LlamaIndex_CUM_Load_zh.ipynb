{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e91244a-3b94-471a-a5de-fc8d931df16a",
   "metadata": {},
   "source": [
    "## Lab5 Create or restore the vectorDB for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3616ed-475d-49c5-b0ad-db8200e77491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92d8b16-b4da-4310-b899-2a0d5f834364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enalbe 780M with ROCm\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '11.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad9c78-c4a6-4610-afe8-48348035e6e7",
   "metadata": {},
   "source": [
    "print(os.environ['HSA_OVERRIDE_GFX_VERSION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e7120a-8bc1-4c70-b61c-2ab6c3d1599b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c78f58e-ab6d-41b6-b80b-a91918397e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+rocm6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e28c16f-2653-4d67-ac89-e72c210a83de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: AMD Radeon Graphics\n",
      "GPU properties: _CudaDeviceProperties(name='AMD Radeon Graphics', major=11, minor=0, gcnArchName='gfx1100', total_memory=16384MB, multi_processor_count=6)\n"
     ]
    }
   ],
   "source": [
    "# Query GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "    print('GPU properties:', torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37ab10d1-c625-4dfb-ac5f-a6cc0b14b91c",
   "metadata": {},
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-embeddings-ollama\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c972c693-898c-4bcc-8a01-d4dfdaa7bc2b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e48e37-ac44-4cfd-b335-6ec39321e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1408895-5635-40b9-971f-92804d74e50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igpu/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Set embedding model\n",
    "# Please download it ahead running this lab by \"ollama pull nomic-embed-text\"\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# Set ollama model\n",
    "Settings.llm = Ollama(model=\"qwen:7b\", request_timeout=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3ebab3-3ec3-41e0-8387-42364ac6df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "if not os.path.exists(\"./chroma_db/CUM_zh_db\"):\n",
    "    # initialize client\n",
    "    db = chromadb.PersistentClient(path=\"./chroma_db/CUM_zh_db\")\n",
    "    # get collection\n",
    "    chroma_collection = db.get_or_create_collection(\"CUM_zh_db\")\n",
    "    # assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # Load data\n",
    "    documents = SimpleDirectoryReader(input_files=[\"../data/QinUM.pdf\"]).load_data()\n",
    "    #print(documents[200])\n",
    "    # Build vector index per-document\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        transformations=[SentenceSplitter(chunk_size=2048, chunk_overlap=100)],\n",
    "    )\n",
    "else:\n",
    "    # initialize client\n",
    "    db = chromadb.PersistentClient(path=\"./chroma_db/CUM_zh_db\")\n",
    "    # get collection\n",
    "    chroma_collection = db.get_or_create_collection(\"CUM_zh_db\")\n",
    "    # assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # load your index from stored vectors\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store, storage_context=storage_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d2e063-20ed-4d8c-ab4d-61857f0e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = index.as_query_engine(streaming=True, response_mode=\"compact\", similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae0951d-d424-4127-af15-cd2bd2107c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Prompt for Car User Manual Q&A\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"你是比亚迪秦汽车的产品专家，请根据用户问题基于产品使用手册进行给出回答和提示。\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"请基于该型号汽车的产品用户手册内容进行回答。\\n\"\n",
    "    \"回答问题时需要给出相关信息在产品用户手册中的页码。\\n\"\n",
    "    \"如果问题超出用户手册之外，请明确告知用户该问题超出手册范围。\\n\"\n",
    "    \"回答内容需准确且精炼。\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am Car User Manual Copilot.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")\n",
    "\n",
    "#prompts_dict = query_engine.get_prompts()\n",
    "#print(list(prompts_dict.keys()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22bec686-8286-4f8c-9201-4aff8483ec2f",
   "metadata": {},
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "response_synthesizer = get_response_synthesizer(structured_answer_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d397b1-e79d-4712-81e0-34fa7fd8fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以通过以下步骤在比亚迪秦汽车上切换DM系统的工作模式：\n",
      "\n",
      "1. **警告**：首先确保车辆安全，如果发生事故，请遵循手册中的应急处理程序。\n",
      "\n",
      "2. **踩制动踏板**：为了降低高压电泄露的风险，你需要踩下刹车踏板。\n",
      "\n",
      "3. **操作电子驻车开关**：接着按下电子驻车开关（通常位于刹车上），这将帮助你停止混合动力系统的运行。\n",
      "\n",
      "4. **使用“P”键**：在面板上找到“P”键（有时候是“SET”或“CONFIG”键），按下它以完成DM系统的工作模式切换。\n",
      "\n",
      "手册中关于这些操作的具体页面可能会因版本和手册更新而有所不同。建议您参照最新的用户手册进行操作。"
     ]
    }
   ],
   "source": [
    "# Query Test 0\n",
    "response = query_engine.query(\"如何切换DM系统工作模式并请给出手册的相关页码？\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0eeeb8-d2ef-4480-8729-e44741c4bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，您提供的信息中没有直接指出介绍仪表盘功能的用户手册页码。通常这类功能介绍会位于车身内部控制系统或者驾驶信息部分的内容页面。\n",
      "\n",
      "根据产品用户手册的一般结构，这部分内容可能会在以下手册页码附近找到：\n",
      "- 361（这可能是一个综合信息章节的页码，包含了多个系统的介绍）；\n",
      "- 如果仪表盘控制是单独一部分，那页码可能会直接标为“4-1充/放电说明”或者类似的页面标识。\n",
      "\n",
      "如果手册中没有明确的页码，那么您可以通过搜索关键词“仪表盘功能”或类似描述来找到相关页面。"
     ]
    }
   ],
   "source": [
    "# Q\n",
    "response = query_engine.query(\"给出介绍仪表盘功能的用户手册页码\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "167c7740-9d6c-4331-a643-e445a8e1f72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，您咨询的产品通风和加热功能开启情况在提供的产品用户手册中并未明确提及。通常情况下，这类功能可能会有单独的开关或者是在特定模式下同时启用。\n",
      "\n",
      "建议您查阅产品的详细使用说明书或直接联系比亚迪汽车授权服务店以获取最准确的信息。"
     ]
    }
   ],
   "source": [
    "# Q\n",
    "response = query_engine.query(\"通风功能与加热功能是否可以同时开启?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816625c1-37c2-46b6-ba94-fead355ed636",
   "metadata": {},
   "source": [
    "## To-Do: Optimization the RAG\n",
    "\n",
    "Refer to Basic Strategies: https://docs.llamaindex.ai/en/v0.10.19/optimizing/basic_strategies/basic_strategies.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
