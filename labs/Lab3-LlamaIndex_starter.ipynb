{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e7120a-8bc1-4c70-b61c-2ab6c3d1599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e28c16f-2653-4d67-ac89-e72c210a83de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: AMD Radeon Graphics\n",
      "GPU properties: _CudaDeviceProperties(name='AMD Radeon Graphics', major=11, minor=0, gcnArchName='gfx1103', total_memory=16384MB, multi_processor_count=6)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "    print('GPU properties:', torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3616ed-475d-49c5-b0ad-db8200e77491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92d8b16-b4da-4310-b899-2a0d5f834364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enalbe 780M with ROCm\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '11.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad9c78-c4a6-4610-afe8-48348035e6e7",
   "metadata": {},
   "source": [
    "print(os.environ['HSA_OVERRIDE_GFX_VERSION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa63a37f-3249-4a4b-a3f9-e3cd358e7297",
   "metadata": {},
   "source": [
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404c95c7-28f2-4653-9486-0ee1ec71a223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igpu/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# nomic embedding model\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44b0693-1c90-4d42-8b5b-0fee3a508a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide any information about what the author did growing up. The context only discusses the author's work and experiences as an adult, specifically related to his startups, Y Combinator, and Viaweb. There is no mention of childhood or teenage years in the provided context.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc008ff-96ad-43ef-a9cd-525e1d362afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
