{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e91244a-3b94-471a-a5de-fc8d931df16a",
   "metadata": {},
   "source": [
    "## Lab6 UCM EN Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3616ed-475d-49c5-b0ad-db8200e77491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92d8b16-b4da-4310-b899-2a0d5f834364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enalbe 780M with ROCm\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '11.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad9c78-c4a6-4610-afe8-48348035e6e7",
   "metadata": {},
   "source": [
    "print(os.environ['HSA_OVERRIDE_GFX_VERSION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e7120a-8bc1-4c70-b61c-2ab6c3d1599b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c78f58e-ab6d-41b6-b80b-a91918397e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+rocm6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e28c16f-2653-4d67-ac89-e72c210a83de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: AMD Radeon Graphics\n",
      "GPU properties: _CudaDeviceProperties(name='AMD Radeon Graphics', major=11, minor=0, gcnArchName='gfx1100', total_memory=16384MB, multi_processor_count=6)\n"
     ]
    }
   ],
   "source": [
    "# Query GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "    print('GPU properties:', torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37ab10d1-c625-4dfb-ac5f-a6cc0b14b91c",
   "metadata": {},
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-embeddings-ollama\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c972c693-898c-4bcc-8a01-d4dfdaa7bc2b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e48e37-ac44-4cfd-b335-6ec39321e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1408895-5635-40b9-971f-92804d74e50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igpu/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Set embedding model\n",
    "# Please download it ahead running this lab by \"ollama pull nomic-embed-text\"\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# Set ollama model\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3ebab3-3ec3-41e0-8387-42364ac6df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "if not os.path.exists(\"./chroma_db/CUM_en_db\"):\n",
    "    # initialize client\n",
    "    db = chromadb.PersistentClient(path=\"./chroma_db/CUM_en_db\")\n",
    "    # get collection\n",
    "    chroma_collection = db.get_or_create_collection(\"CUM_en_db\")\n",
    "    # assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # Load data\n",
    "    documents = SimpleDirectoryReader(input_files=[\"../data/FordUM.pdf\"]).load_data()\n",
    "    # Build vector index per-document\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=50)],\n",
    "    )\n",
    "else:\n",
    "    # initialize client\n",
    "    db = chromadb.PersistentClient(path=\"./chroma_db/CUM_en_db\")\n",
    "    # get collection\n",
    "    chroma_collection = db.get_or_create_collection(\"CUM_en_db\")\n",
    "    # assign chroma as the vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # load your index from stored vectors\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store, storage_context=storage_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d2e063-20ed-4d8c-ab4d-61857f0e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = index.as_query_engine(streaming=True, response_mode=\"compact\", similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9270a6c3-06ec-40fc-ac18-3c629216940f",
   "metadata": {},
   "source": [
    "# Updating Prompt for Car User Manual Q&A\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are proudct expert of car and very faimilay with car user manual and provide guide to the end user.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"answer the question according to the care user manual with page number from the Index Table.\\n\"\n",
    "    \"if the question is not releate with car user manual, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"if the question is in chinese, please transclate chinese to english in advance\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am Car User Manual Copilot.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")\n",
    "\n",
    "#prompts_dict = query_engine.get_prompts()\n",
    "#print(list(prompts_dict.keys()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22bec686-8286-4f8c-9201-4aff8483ec2f",
   "metadata": {},
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "response_synthesizer = get_response_synthesizer(structured_answer_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d397b1-e79d-4712-81e0-34fa7fd8fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on the provided context information, I would suggest that pages 127-128 or 365-366 might have details about Exterior Lighting Control. However, since the exact page numbers are not explicitly mentioned in the given context, it's possible that the relevant information could be spread across multiple pages or even on a different document altogether."
     ]
    }
   ],
   "source": [
    "# Query Test 0\n",
    "response = query_engine.query(\"Which pages could get the detials about Exterior Lighting Control?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d0eeeb8-d2ef-4480-8729-e44741c4bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m happy to help! However, I must point out that the provided context information does not contain any phone numbers or details related to the customer relationship center in China. The context appears to be focused on the features and controls of a Ford vehicle, specifically the F-150 Lightning.\n",
      "\n",
      "As such, I cannot provide an answer to your query based on the given context information. If you could provide more relevant context or clarify what you are looking for, I'll do my best to assist you."
     ]
    }
   ],
   "source": [
    "# Q\n",
    "response = query_engine.query(\"find out the phone number of the customer relationship center in China from the Car User Manual\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167c7740-9d6c-4331-a643-e445a8e1f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to the provided context, the steps to switch the daytime running lamps on and off are:\n",
      "\n",
      "1. Press Settings on the touch screen.\n",
      "2. Press Vehicle Settings.\n",
      "3. Press Lighting.\n",
      "4. Switch Daytime Running Lights on or off.\n",
      "\n",
      "This information can be found on pages 115 and 127 of the Ford F-150 Lightning manual."
     ]
    }
   ],
   "source": [
    "# Q\n",
    "response = query_engine.query(\"What's the steps to SWITCHING THE DAYTIME RUNNING LAMPS ON AND OFF and which pages show it?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816625c1-37c2-46b6-ba94-fead355ed636",
   "metadata": {},
   "source": [
    "## To-Do: Optimization the RAG\n",
    "\n",
    "Refer to Basic Strategies: https://docs.llamaindex.ai/en/v0.10.19/optimizing/basic_strategies/basic_strategies.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
